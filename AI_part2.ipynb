{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woR5HIBJaub7",
        "outputId": "fd5cf8ec-2959-4a0a-ff38-ea157b54be18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain sentence-transformers faiss-cpu \"transformers[torch]\" langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq11Cxg7bQTt",
        "outputId": "e1151467-d2e1-49c1-b119-be9c34733f4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Project directory: /content/drive/MyDrive/AI_Assignment\n",
            "âœ… Stage 1 outputs: /content/drive/MyDrive/AI_Assignment/stage1_outputs\n",
            "âœ… Stage 2 outputs: /content/drive/MyDrive/AI_Assignment/stage2_outputs\n",
            "\n",
            "ğŸ“‚ Directory structure ready!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"ğŸ“ Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "PROJECT_DIR = '/content/drive/MyDrive/AI_Assignment'\n",
        "STAGE1_OUTPUT_DIR = f\"{PROJECT_DIR}/stage1_outputs\"\n",
        "STAGE2_DIR = f\"{PROJECT_DIR}/stage2_outputs\"\n",
        "\n",
        "\n",
        "os.makedirs(STAGE2_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"âœ… Project directory: {PROJECT_DIR}\")\n",
        "print(f\"âœ… Stage 1 outputs: {STAGE1_OUTPUT_DIR}\")\n",
        "print(f\"âœ… Stage 2 outputs: {STAGE2_DIR}\")\n",
        "print(\"\\nğŸ“‚ Directory structure ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d465o-cvedA-",
        "outputId": "6a3b8eb4-a099-4120-c8a4-499626b6e6ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ–¥ï¸  Using device: cuda\n",
            "   GPU: Tesla T4\n",
            "   Memory: 15.83 GB\n",
            "\n",
            "âœ… All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "from transformers import pipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ğŸ–¥ï¸  Using device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"   âš ï¸  WARNING: GPU not detected!\")\n",
        "    print(\"   Go to: Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ T4 GPU\")\n",
        "    print(\"   Then: Runtime â†’ Restart runtime\")\n",
        "\n",
        "print(\"\\nâœ… All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJp0Sg4-g0YN",
        "outputId": "283117d1-4fb9-4880-976b-250d8333e754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files in stage1_outputs:\n",
            "  - training_metrics.txt\n",
            "  - evaluation_metrics.txt\n",
            "  - stage1_predictions.csv\n",
            "  - confusion_matrix.png\n",
            "  - gradcam_img_0_true_anger.png\n",
            "  - gradcam_img_1_true_surprise.png\n",
            "  - gradcam_img_2_true_neutral.png\n",
            "  - gradcam_img_3_true_sadness.png\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "stage1_dir = '/content/drive/MyDrive/AI_Assignment/stage1_outputs'\n",
        "if os.path.exists(stage1_dir):\n",
        "    print(\"Files in stage1_outputs:\")\n",
        "    for f in os.listdir(stage1_dir):\n",
        "        print(f\"  - {f}\")\n",
        "else:\n",
        "    print(\"Folder doesn't exist!\")\n",
        "    print(\"\\nChecking parent folder:\")\n",
        "    parent = '/content/drive/MyDrive/AI_Assignment'\n",
        "    if os.path.exists(parent):\n",
        "        print(\"Files in AI_Assignment:\")\n",
        "        for f in os.listdir(parent):\n",
        "            print(f\"  - {f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3clNbmSpfr9O",
        "outputId": "9beba5ff-64b9-4b60-bcc7-29e1311f5b25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TASK 1: DATA GENERATION\n",
            "============================================================\n",
            "\n",
            "ğŸ“‚ Loading predictions from: /content/drive/MyDrive/AI_Assignment/stage1_outputs/stage1_predictions.csv\n",
            "âœ… Loaded 3589 predictions\n",
            "\n",
            "ğŸ“Š Columns: ['true_label_id', 'predicted_label_id', 'true_label_name', 'predicted_label_name']\n",
            "\n",
            "ğŸ” First few rows:\n",
            "   true_label_id  predicted_label_id true_label_name predicted_label_name\n",
            "0              0                   0           anger                anger\n",
            "1              5                   4        surprise              sadness\n",
            "2              6                   2         neutral                 fear\n",
            "3              4                   4         sadness              sadness\n",
            "4              2                   4            fear              sadness\n",
            "\n",
            "âœ‚ï¸  Sampled 500 predictions for Stage 2\n",
            "\n",
            "ğŸ“ Defining review templates...\n",
            "âœ… Loaded 7 emotion categories with 70 total templates\n",
            "\n",
            "ğŸ”„ Generating synthetic reviews...\n",
            "âœ… Successfully generated 500 synthetic reviews\n",
            "   Unique combinations used: 60\n",
            "ğŸ’¾ Saved reviews to: /content/drive/MyDrive/AI_Assignment/stage2_outputs/generated_reviews.csv\n",
            "\n",
            "ğŸ“Š Emotion Distribution in Generated Reviews:\n",
            "   happiness: 128 (25.6%)\n",
            "   neutral: 120 (24.0%)\n",
            "   sadness: 87 (17.4%)\n",
            "   anger: 58 (11.6%)\n",
            "   surprise: 54 (10.8%)\n",
            "   fear: 53 (10.6%)\n",
            "\n",
            "ğŸ“ Sample Generated Reviews:\n",
            "\n",
            "  [ANGER]\n",
            "  \"Rage-inducing false advertising and broken promises.\"\n",
            "\n",
            "  [HAPPINESS]\n",
            "  \"Love everything, couldn't ask for better!\"\n",
            "\n",
            "  [SADNESS]\n",
            "  \"Down about experience, expected better results.\"\n",
            "\n",
            "============================================================\n",
            "âœ… TASK 1 COMPLETE: Data Generation\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TASK 1: DATA GENERATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "predictions_path = f\"{STAGE1_OUTPUT_DIR}/stage1_predictions.csv\"\n",
        "print(f\"\\nğŸ“‚ Loading predictions from: {predictions_path}\")\n",
        "\n",
        "try:\n",
        "    pred_df = pd.read_csv(predictions_path)\n",
        "    print(f\"âœ… Loaded {len(pred_df)} predictions\")\n",
        "    print(f\"\\nğŸ“Š Columns: {list(pred_df.columns)}\")\n",
        "    print(f\"\\nğŸ” First few rows:\")\n",
        "    print(pred_df.head())\n",
        "\n",
        "    \n",
        "    if len(pred_df) > 500:\n",
        "        sample_df = pred_df.sample(n=500, random_state=42)\n",
        "        print(f\"\\nâœ‚ï¸  Sampled 500 predictions for Stage 2\")\n",
        "    else:\n",
        "        sample_df = pred_df\n",
        "        print(f\"\\nğŸ“ Using all {len(sample_df)} predictions\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ ERROR: Cannot find predictions file at {predictions_path}\")\n",
        "    print(\"\\nPlease ensure Stage 1 is complete and the file exists.\")\n",
        "    raise\n",
        "\n",
        "\n",
        "print(\"\\nğŸ“ Defining review templates...\")\n",
        "templates = {\n",
        "    'anger': [\n",
        "        \"Absolutely furious with purchase quality issues.\",\n",
        "        \"Worst customer service experience ever encountered.\",\n",
        "        \"Completely dissatisfied, demanding immediate refund now.\",\n",
        "        \"Outrageous product defects, unacceptable standards.\",\n",
        "        \"This made me livid, terrible quality control.\",\n",
        "        \"Infuriating delivery delays and poor communication.\",\n",
        "        \"Rage-inducing false advertising and broken promises.\",\n",
        "        \"Extremely upset, product broke within days.\",\n",
        "        \"Beyond angry, company refuses responsibility.\",\n",
        "        \"Angry doesn't describe my frustration adequately.\"\n",
        "    ],\n",
        "    'happiness': [\n",
        "        \"Absolutely thrilled with exceptional product quality!\",\n",
        "        \"Best purchase decision made this year!\",\n",
        "        \"Exceeded expectations in every possible way!\",\n",
        "        \"Delighted by outstanding customer service experience!\",\n",
        "        \"Perfect product, brings daily joy and satisfaction!\",\n",
        "        \"Highly recommend, worth every single penny!\",\n",
        "        \"Love everything, couldn't ask for better!\",\n",
        "        \"Ecstatic about fast shipping and quality!\",\n",
        "        \"Overjoyed with results, exactly as advertised!\",\n",
        "        \"Happiest customer, will definitely buy again!\"\n",
        "    ],\n",
        "    'sadness': [\n",
        "        \"Disappointed by poor quality, money wasted.\",\n",
        "        \"Had hopes but product underperformed significantly.\",\n",
        "        \"Really sad experience, failed expectations completely.\",\n",
        "        \"Heartbroken by misleading product descriptions online.\",\n",
        "        \"Feeling let down, not worth price paid.\",\n",
        "        \"Melancholy purchase, wish never bought it.\",\n",
        "        \"Down about experience, expected better results.\",\n",
        "        \"Dejected by lack of support after sale.\",\n",
        "        \"Sorrowful decision, complete buyer's remorse now.\",\n",
        "        \"Disappointed tears, absolute waste of money.\"\n",
        "    ],\n",
        "    'surprise': [\n",
        "        \"Wow, totally unexpected excellent performance results!\",\n",
        "        \"Genuinely shocked by superior quality levels!\",\n",
        "        \"Surprising excellence, never anticipated this!\",\n",
        "        \"Astonishing value, exceeded wildest dreams!\",\n",
        "        \"Caught off guard by phenomenal features!\",\n",
        "        \"What shock, blown away completely!\",\n",
        "        \"Unexpected brilliance in design and function!\",\n",
        "        \"Startling effectiveness, pleasantly amazed daily!\",\n",
        "        \"Incredible surprise beyond initial expectations!\",\n",
        "        \"Jaw-dropping quality, never saw coming!\"\n",
        "    ],\n",
        "    'fear': [\n",
        "        \"Worried about serious safety concerns here.\",\n",
        "        \"Frighteningly poor build quality risks.\",\n",
        "        \"Nerve-wracking unreliability causes daily stress.\",\n",
        "        \"Genuinely concerned about potential hazards.\",\n",
        "        \"Terrifying defects could cause injuries.\",\n",
        "        \"Anxious using product, seems dangerous.\",\n",
        "        \"Dreadful quality, fearful of failures.\",\n",
        "        \"Alarming issues, scared it will break.\",\n",
        "        \"Intimidating risks, worried about consequences.\",\n",
        "        \"Frightened by obvious manufacturing flaws.\"\n",
        "    ],\n",
        "    'disgust': [\n",
        "        \"Disgusting lack of basic quality standards.\",\n",
        "        \"Repulsed by terrible craftsmanship throughout.\",\n",
        "        \"Awful condition, completely unacceptable filth.\",\n",
        "        \"Revolting product arrived damaged and dirty.\",\n",
        "        \"Absolutely repellent, can't believe standards.\",\n",
        "        \"Nauseating poor quality offends customers.\",\n",
        "        \"Repugnant disregard for basic standards.\",\n",
        "        \"Sickening experience from beginning to end.\",\n",
        "        \"Abhorrent quality control completely absent.\",\n",
        "        \"Vile product, utterly disgusted entirely.\"\n",
        "    ],\n",
        "    'neutral': [\n",
        "        \"Product performs adequately for basic needs.\",\n",
        "        \"Average quality, meets minimum expectations only.\",\n",
        "        \"Neutral opinion, neither impressed nor disappointed.\",\n",
        "        \"Standard functionality, works as described.\",\n",
        "        \"Decent purchase, gets job done sufficiently.\",\n",
        "        \"Acceptable quality level for price paid.\",\n",
        "        \"Mediocre experience overall, nothing special.\",\n",
        "        \"Satisfactory but unremarkable in all aspects.\",\n",
        "        \"Ordinary quality, functions properly enough.\",\n",
        "        \"Adequate product serving intended purpose.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(f\"âœ… Loaded {len(templates)} emotion categories with {sum(len(v) for v in templates.values())} total templates\")\n",
        "\n",
        "\n",
        "print(f\"\\nğŸ”„ Generating synthetic reviews...\")\n",
        "reviews_docs = []\n",
        "review_data = []\n",
        "used_combinations = set() \n",
        "\n",
        "for idx, row in sample_df.iterrows():\n",
        "    emotion = row['predicted_label_name']\n",
        "\n",
        "   \n",
        "    max_attempts = 10\n",
        "    for attempt in range(max_attempts):\n",
        "        template_idx = np.random.randint(0, len(templates[emotion]))\n",
        "        combo = (emotion, template_idx)\n",
        "        if combo not in used_combinations or attempt == max_attempts - 1:\n",
        "            used_combinations.add(combo)\n",
        "            break\n",
        "\n",
        "    review_text = templates[emotion][template_idx]\n",
        "\n",
        "   \n",
        "    doc = Document(\n",
        "        page_content=review_text,\n",
        "        metadata={\n",
        "            \"emotion\": emotion,\n",
        "            \"true_emotion\": row['true_label_name']\n",
        "        }\n",
        "    )\n",
        "    reviews_docs.append(doc)\n",
        "\n",
        "    \n",
        "    review_data.append({\n",
        "        'predicted_emotion': emotion,\n",
        "        'true_emotion': row['true_label_name'],\n",
        "        'review_text': review_text\n",
        "    })\n",
        "\n",
        "print(f\"âœ… Successfully generated {len(reviews_docs)} synthetic reviews\")\n",
        "print(f\"   Unique combinations used: {len(used_combinations)}\")\n",
        "\n",
        "\n",
        "reviews_csv_path = f\"{STAGE2_DIR}/generated_reviews.csv\"\n",
        "reviews_df = pd.DataFrame(review_data)\n",
        "reviews_df.to_csv(reviews_csv_path, index=False)\n",
        "print(f\"ğŸ’¾ Saved reviews to: {reviews_csv_path}\")\n",
        "\n",
        "\n",
        "print(f\"\\nğŸ“Š Emotion Distribution in Generated Reviews:\")\n",
        "emotion_counts = reviews_df['predicted_emotion'].value_counts()\n",
        "for emotion, count in emotion_counts.items():\n",
        "    print(f\"   {emotion}: {count} ({count/len(reviews_df)*100:.1f}%)\")\n",
        "\n",
        "\n",
        "print(f\"\\nğŸ“ Sample Generated Reviews:\")\n",
        "for emotion in list(templates.keys())[:3]:\n",
        "    emotion_samples = reviews_df[reviews_df['predicted_emotion'] == emotion]\n",
        "    if len(emotion_samples) > 0:\n",
        "        sample = emotion_samples.iloc[0]\n",
        "        print(f\"\\n  [{emotion.upper()}]\")\n",
        "        print(f\"  \\\"{sample['review_text']}\\\"\")\n",
        "    else:\n",
        "        print(f\"\\n  [{emotion.upper()}]\")\n",
        "        print(f\"  (No samples in this batch)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… TASK 1 COMPLETE: Data Generation\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o44j2obffzMl",
        "outputId": "422650ce-5660-42b6-abd1-369487b52220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TASK 2: EMBEDDING & VECTOR STORE SETUP\n",
            "============================================================\n",
            "\n",
            "ğŸ”„ Loading sentence-transformer embedding model...\n",
            "   Model: sentence-transformers/all-MiniLM-L6-v2\n",
            "   Device: cuda\n",
            "âœ… Embedding model loaded successfully\n",
            "\n",
            "ğŸ”„ Creating FAISS vector database from 500 documents...\n",
            "   This will embed all reviews using the GPU...\n",
            "âœ… Vector database created in 0.19 seconds\n",
            "   Embedding speed: 2696.6 documents/second\n",
            "\n",
            "ğŸ’¾ Saving FAISS index to: /content/drive/MyDrive/AI_Assignment/stage2_outputs/faiss_index\n",
            "âœ… FAISS index saved successfully\n",
            "\n",
            "ğŸ” Testing similarity search...\n",
            "\n",
            "   Query: \"Tell me about angry customers\"\n",
            "   Top 3 Results:\n",
            "\n",
            "   1. [ANGER]\n",
            "      \"Rage-inducing false advertising and broken promises....\"\n",
            "\n",
            "   2. [ANGER]\n",
            "      \"Rage-inducing false advertising and broken promises....\"\n",
            "\n",
            "   3. [ANGER]\n",
            "      \"Rage-inducing false advertising and broken promises....\"\n",
            "\n",
            "============================================================\n",
            "âœ… TASK 2 COMPLETE: Embedding & Vector Store Setup\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TASK 2: EMBEDDING & VECTOR STORE SETUP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "print(\"\\nğŸ”„ Loading sentence-transformer embedding model...\")\n",
        "print(\"   Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "print(\"âœ… Embedding model loaded successfully\")\n",
        "\n",
        "\n",
        "print(f\"\\nğŸ”„ Creating FAISS vector database from {len(reviews_docs)} documents...\")\n",
        "print(\"   This will embed all reviews using the GPU...\")\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "db = FAISS.from_documents(reviews_docs, embeddings)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"âœ… Vector database created in {elapsed:.2f} seconds\")\n",
        "print(f\"   Embedding speed: {len(reviews_docs)/elapsed:.1f} documents/second\")\n",
        "\n",
        "\n",
        "faiss_index_path = f\"{STAGE2_DIR}/faiss_index\"\n",
        "print(f\"\\nğŸ’¾ Saving FAISS index to: {faiss_index_path}\")\n",
        "db.save_local(faiss_index_path)\n",
        "print(\"âœ… FAISS index saved successfully\")\n",
        "\n",
        "\n",
        "print(\"\\nğŸ” Testing similarity search...\")\n",
        "test_query = \"Tell me about angry customers\"\n",
        "results = db.similarity_search(test_query, k=3)\n",
        "\n",
        "print(f\"\\n   Query: \\\"{test_query}\\\"\")\n",
        "print(f\"   Top 3 Results:\")\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"\\n   {i}. [{doc.metadata['emotion'].upper()}]\")\n",
        "    print(f\"      \\\"{doc.page_content[:100]}...\\\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… TASK 2 COMPLETE: Embedding & Vector Store Setup\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvJKMctSgiVX",
        "outputId": "cec9e4eb-9a1d-4b96-f17d-d8dcfeef2745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TASK 3: RAG QUERY SYSTEM USING LANGCHAIN\n",
            "============================================================\n",
            "\n",
            "ğŸ”„ Loading open-source LLM for RAG pipeline...\n",
            "   Model: google/flan-t5-base\n",
            "   Device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… LLM loaded successfully\n",
            "\n",
            "ğŸ“ Creating custom prompt template...\n",
            "\n",
            "ğŸ”— Building RAG chain with custom diverse retriever...\n",
            "âœ… RAG pipeline is ready!\n",
            "   Retriever: Custom diverse retriever (k=4, forced uniqueness)\n",
            "   LLM: google/flan-t5-base\n",
            "   Chain type: Stuff (all docs in context)\n",
            "\n",
            "============================================================\n",
            "âœ… TASK 3 COMPLETE: RAG Query System Created\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TASK 3: RAG QUERY SYSTEM USING LANGCHAIN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "print(\"\\nğŸ”„ Loading open-source LLM for RAG pipeline...\")\n",
        "print(\"   Model: google/flan-t5-base\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "llm_pipeline = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    max_length=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.3,\n",
        "    top_p=0.9,\n",
        "    device=0 if device == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "print(\"âœ… LLM loaded successfully\")\n",
        "\n",
        "# Create improved prompt template\n",
        "print(\"\\nğŸ“ Creating custom prompt template...\")\n",
        "prompt_template = \"\"\"Based on the customer reviews below, provide a clear and concise summary that directly answers the question.\n",
        "\n",
        "Customer Reviews:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Provide a brief summary (2-3 sentences maximum):\"\"\"\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "\n",
        "from langchain.schema.retriever import BaseRetriever\n",
        "from langchain.callbacks.manager import CallbackManagerForRetrieverRun\n",
        "from typing import List\n",
        "\n",
        "class DiverseRetriever(BaseRetriever):\n",
        "    vectorstore: object\n",
        "    k: int = 4\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def _get_relevant_documents(\n",
        "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun = None\n",
        "    ) -> List:\n",
        "     \n",
        "        candidates = self.vectorstore.similarity_search(query, k=20)\n",
        "\n",
        "       \n",
        "        selected = []\n",
        "        seen_texts = set()\n",
        "\n",
        "        for doc in candidates:\n",
        "            text_start = doc.page_content[:50]  # First 50 chars\n",
        "            if text_start not in seen_texts and len(selected) < self.k:\n",
        "                selected.append(doc)\n",
        "                seen_texts.add(text_start)\n",
        "\n",
        "       \n",
        "        if len(selected) < self.k:\n",
        "            for doc in candidates:\n",
        "                if len(selected) >= self.k:\n",
        "                    break\n",
        "                if doc not in selected:\n",
        "                    selected.append(doc)\n",
        "\n",
        "        return selected[:self.k]\n",
        "\n",
        "\n",
        "print(\"\\nğŸ”— Building RAG chain with custom diverse retriever...\")\n",
        "diverse_retriever = DiverseRetriever(vectorstore=db, k=4)\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=diverse_retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": PROMPT}\n",
        ")\n",
        "\n",
        "print(\"âœ… RAG pipeline is ready!\")\n",
        "print(f\"   Retriever: Custom diverse retriever (k=4, forced uniqueness)\")\n",
        "print(f\"   LLM: google/flan-t5-base\")\n",
        "print(f\"   Chain type: Stuff (all docs in context)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… TASK 3 COMPLETE: RAG Query System Created\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZegMg4hgnM_",
        "outputId": "b96386d8-a128-4e24-9f9a-60188ddd030d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TASK 4: SENTIMENT ANALYSIS INTEGRATION\n",
            "============================================================\n",
            "\n",
            "ğŸ”„ Loading 3-label sentiment analysis model...\n",
            "   Model: cardiffnlp/twitter-roberta-base-sentiment-latest\n",
            "   Device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Sentiment analysis model loaded successfully\n",
            "\n",
            "ğŸ” Testing sentiment analysis...\n",
            "\n",
            "   Text: \"I am so happy with this purchase!\"\n",
            "   Sentiment: positive (confidence: 0.989)\n",
            "\n",
            "   Text: \"This is absolutely terrible and disgusting.\"\n",
            "   Sentiment: negative (confidence: 0.938)\n",
            "\n",
            "   Text: \"The product is fine, nothing special.\"\n",
            "   Sentiment: neutral (confidence: 0.517)\n",
            "\n",
            "============================================================\n",
            "âœ… TASK 4 COMPLETE: Sentiment Analysis Integration\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TASK 4: SENTIMENT ANALYSIS INTEGRATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "print(\"\\nğŸ”„ Loading 3-label sentiment analysis model...\")\n",
        "print(\"   Model: cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "sentiment_pipe = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    device=0 if device == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "\n",
        "sentiment_labels = {\n",
        "    \"LABEL_0\": \"NEGATIVE\",\n",
        "    \"LABEL_1\": \"NEUTRAL\",\n",
        "    \"LABEL_2\": \"POSITIVE\"\n",
        "}\n",
        "\n",
        "print(\"âœ… Sentiment analysis model loaded successfully\")\n",
        "\n",
        "\n",
        "print(\"\\nğŸ” Testing sentiment analysis...\")\n",
        "test_texts = [\n",
        "    \"I am so happy with this purchase!\",\n",
        "    \"This is absolutely terrible and disgusting.\",\n",
        "    \"The product is fine, nothing special.\"  \n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    sentiment = sentiment_pipe(text)[0]\n",
        "    label_name = sentiment_labels.get(sentiment['label'], sentiment['label'])\n",
        "\n",
        "    print(f\"\\n   Text: \\\"{text}\\\"\")\n",
        "    print(f\"   Sentiment: {label_name} (confidence: {sentiment['score']:.3f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… TASK 4 COMPLETE: Sentiment Analysis Integration\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aukgz4vihFe1",
        "outputId": "7631c825-a6cf-4ab1-c9c8-4cb5716f0aab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "DELIVERABLE: SAMPLE QUERY 1\n",
            "======================================================================\n",
            "\n",
            "ğŸ” Query: \"What are customers angry about?\"\n",
            "\n",
            "ğŸ”„ Processing query through RAG pipeline...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“ RAG SUMMARY (Generated by LLM):\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Rage-inducing false advertising and broken promises. Beyond angry, company refuses responsibility. Worst customer service experience ever encountered. Absolutely furious with purchase quality issues.\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“š RETRIEVED DOCUMENTS & SENTIMENT ANALYSIS:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“„ Document 1:\n",
            "   Emotion: ANGER\n",
            "   Review: \"Rage-inducing false advertising and broken promises.\"\n",
            "   Sentiment: negative (Score: 0.905\n",
            "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“„ Document 2:\n",
            "   Emotion: ANGER\n",
            "   Review: \"Beyond angry, company refuses responsibility.\"\n",
            "   Sentiment: negative (Score: 0.933\n",
            "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“„ Document 3:\n",
            "   Emotion: ANGER\n",
            "   Review: \"Worst customer service experience ever encountered.\"\n",
            "   Sentiment: negative (Score: 0.948\n",
            "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“„ Document 4:\n",
            "   Emotion: ANGER\n",
            "   Review: \"Absolutely furious with purchase quality issues.\"\n",
            "   Sentiment: negative (Score: 0.934\n",
            "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "======================================================================\n",
            "âœ… Query 1 Complete\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DELIVERABLE: SAMPLE QUERY 1\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "query_1 = \"What are customers angry about?\"\n",
        "print(f\"\\nğŸ” Query: \\\"{query_1}\\\"\\n\")\n",
        "print(\"ğŸ”„ Processing query through RAG pipeline...\\n\")\n",
        "\n",
        "\n",
        "result_1 = qa_chain({\"query\": query_1})\n",
        "\n",
        "\n",
        "print(\"â”€\" * 70)\n",
        "print(\"ğŸ“ RAG SUMMARY (Generated by LLM):\")\n",
        "print(\"â”€\" * 70)\n",
        "print(result_1['result'])\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"â”€\" * 70)\n",
        "print(\"ğŸ“š RETRIEVED DOCUMENTS & SENTIMENT ANALYSIS:\")\n",
        "print(\"â”€\" * 70)\n",
        "\n",
        "if not result_1['source_documents']:\n",
        "    print(\"âŒ No relevant documents found.\")\n",
        "else:\n",
        "  for i, doc in enumerate(result_1['source_documents'], 1):\n",
        "    doc_text = doc.page_content\n",
        "    doc_emotion = doc.metadata.get('emotion', 'unknown')\n",
        "\n",
        "   \n",
        "    sentiment = sentiment_pipe(doc_text)[0]\n",
        "    label_name = sentiment_labels.get(sentiment['label'], sentiment['label'])\n",
        "\n",
        "    print(f\"\\nğŸ“„ Document {i}:\")\n",
        "    print(f\"   Emotion: {doc_emotion.upper()}\")\n",
        "    print(f\"   Review: \\\"{doc_text}\\\"\")\n",
        "    print(f\"   Sentiment: {label_name} (Score: {sentiment['score']:.3f}\")\n",
        "    print(\"   \" + \"â”€\" * 66)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… Query 1 Complete\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-i1cUKhhMcs",
        "outputId": "acd348df-288c-48be-f85b-90df0a109d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "DELIVERABLE: SAMPLE QUERY 2\n",
            "======================================================================\n",
            "\n",
            "ğŸ” Query: \"Tell me what the happy customers are saying\"\n",
            "\n",
            "ğŸ”„ Processing query through RAG pipeline...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“ RAG SUMMARY (Generated by LLM):\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Delighted by outstanding customer service experience! Happiest customer, will definitely buy again! Delighted by outstanding customer service experience!\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“š RETRIEVED DOCUMENTS & SENTIMENT ANALYSIS:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“„ Document 1:\n",
            "   Emotion: HAPPINESS\n",
            "   Review: \"Delighted by outstanding customer service experience!\"\n",
            "   Sentiment: positive (Score: 0.984\n",
            "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“„ Document 2:\n",
            "   Emotion: HAPPINESS\n",
            "   Review: \"Happiest customer, will definitely buy again!\"\n",
            "   Sentiment: positive (Score: 0.987\n",
            "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“„ Document 3:\n",
            "   Emotion: HAPPINESS\n",
            "   Review: \"Delighted by outstanding customer service experience!\"\n",
            "   Sentiment: positive (Score: 0.984\n",
            "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“„ Document 4:\n",
            "   Emotion: HAPPINESS\n",
            "   Review: \"Delighted by outstanding customer service experience!\"\n",
            "   Sentiment: positive (Score: 0.984\n",
            "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "======================================================================\n",
            "âœ… Query 2 Complete\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DELIVERABLE: SAMPLE QUERY 2\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "query_2 = \"Tell me what the happy customers are saying\"\n",
        "print(f\"\\nğŸ” Query: \\\"{query_2}\\\"\\n\")\n",
        "print(\"ğŸ”„ Processing query through RAG pipeline...\\n\")\n",
        "\n",
        "\n",
        "result_2 = qa_chain({\"query\": query_2})\n",
        "\n",
        "\n",
        "print(\"â”€\" * 70)\n",
        "print(\"ğŸ“ RAG SUMMARY (Generated by LLM):\")\n",
        "print(\"â”€\" * 70)\n",
        "print(result_2['result'])\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"â”€\" * 70)\n",
        "print(\"ğŸ“š RETRIEVED DOCUMENTS & SENTIMENT ANALYSIS:\")\n",
        "print(\"â”€\" * 70)\n",
        "\n",
        "if not result_2['source_documents']:\n",
        "    print(\"âŒ No relevant documents found.\")\n",
        "else:\n",
        "  for i, doc in enumerate(result_2['source_documents'], 1):\n",
        "    doc_text = doc.page_content\n",
        "    doc_emotion = doc.metadata.get('emotion', 'unknown')\n",
        "\n",
        "\n",
        "    sentiment = sentiment_pipe(doc_text)[0]\n",
        "    label_name = sentiment_labels.get(sentiment['label'], sentiment['label'])\n",
        "\n",
        "    print(f\"\\nğŸ“„ Document {i}:\")\n",
        "    print(f\"   Emotion: {doc_emotion.upper()}\")\n",
        "    print(f\"   Review: \\\"{doc_text}\\\"\")\n",
        "    print(f\"   Sentiment: {label_name} (Score: {sentiment['score']:.3f}\")\n",
        "    print(\"   \" + \"â”€\" * 66)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… Query 2 Complete\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLGKjDWEhPIk",
        "outputId": "2bd7a2a0-a234-4ec3-fc3f-edffa720bebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "DELIVERABLE: SAMPLE QUERY 3 (BONUS)\n",
            "======================================================================\n",
            "\n",
            "ğŸ” Query: \"Are there any surprised or shocked customers?\"\n",
            "\n",
            "ğŸ”„ Processing query through RAG pipeline...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“ RAG SUMMARY (Generated by LLM):\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Customer reviews: Incredible surprise beyond initial expectations! Delighted by outstanding customer service experience! Genuinely shocked by superior quality levels!\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“š RETRIEVED DOCUMENTS & SENTIMENT ANALYSIS:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“„ Document 1:\n",
            "   Emotion: SURPRISE\n",
            "   Review: \"Incredible surprise beyond initial expectations!\"\n",
            "   Sentiment: positive (Score: 0.973\n",
            "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“„ Document 2:\n",
            "   Emotion: HAPPINESS\n",
            "   Review: \"Delighted by outstanding customer service experience!\"\n",
            "   Sentiment: positive (Score: 0.984\n",
            "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“„ Document 3:\n",
            "   Emotion: SURPRISE\n",
            "   Review: \"Genuinely shocked by superior quality levels!\"\n",
            "   Sentiment: positive (Score: 0.483\n",
            "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“„ Document 4:\n",
            "   Emotion: SURPRISE\n",
            "   Review: \"Incredible surprise beyond initial expectations!\"\n",
            "   Sentiment: positive (Score: 0.973\n",
            "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "======================================================================\n",
            "âœ… Query 3 Complete\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DELIVERABLE: SAMPLE QUERY 3 (BONUS)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "query_3 = \"Are there any surprised or shocked customers?\"\n",
        "print(f\"\\nğŸ” Query: \\\"{query_3}\\\"\\n\")\n",
        "print(\"ğŸ”„ Processing query through RAG pipeline...\\n\")\n",
        "\n",
        "\n",
        "result_3 = qa_chain({\"query\": query_3})\n",
        "\n",
        "\n",
        "print(\"â”€\" * 70)\n",
        "print(\"ğŸ“ RAG SUMMARY (Generated by LLM):\")\n",
        "print(\"â”€\" * 70)\n",
        "print(result_3['result'])\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"â”€\" * 70)\n",
        "print(\"ğŸ“š RETRIEVED DOCUMENTS & SENTIMENT ANALYSIS:\")\n",
        "print(\"â”€\" * 70)\n",
        "\n",
        "if not result_3['source_documents']:\n",
        "    print(\"âŒ No relevant documents found.\")\n",
        "else:\n",
        "  for i, doc in enumerate(result_3['source_documents'], 1):\n",
        "    doc_text = doc.page_content\n",
        "    doc_emotion = doc.metadata.get('emotion', 'unknown')\n",
        "\n",
        "  \n",
        "    sentiment = sentiment_pipe(doc_text)[0]\n",
        "    label_name = sentiment_labels.get(sentiment['label'], sentiment['label'])\n",
        "\n",
        "    print(f\"\\nğŸ“„ Document {i}:\")\n",
        "    print(f\"   Emotion: {doc_emotion.upper()}\")\n",
        "    print(f\"   Review: \\\"{doc_text}\\\"\")\n",
        "    print(f\"   Sentiment: {label_name} (Score: {sentiment['score']:.3f}\")\n",
        "    print(\"   \" + \"â”€\" * 66)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… Query 3 Complete\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZTlTfC7hRPR",
        "outputId": "cc622464-16fd-44dc-f10d-ac6cd11ea300"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "SAVING QUERY RESULTS\n",
            "============================================================\n",
            "\n",
            "ğŸ’¾ Query results saved to: /content/drive/MyDrive/AI_Assignment/stage2_outputs/query_results.csv\n",
            "   Total records: 12\n",
            "\n",
            "ğŸ“Š Results Summary:\n",
            "   Total queries: 3\n",
            "   Documents per query: 4\n",
            "   Total retrieved documents: 12\n",
            "\n",
            "âœ… Query results saved successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING QUERY RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "all_results = [\n",
        "    {'query': query_1, 'result': result_1},\n",
        "    {'query': query_2, 'result': result_2},\n",
        "    {'query': query_3, 'result': result_3}\n",
        "]\n",
        "\n",
        "\n",
        "query_results = []\n",
        "\n",
        "for idx, item in enumerate(all_results, 1):\n",
        "    query = item['query']\n",
        "    result = item['result']\n",
        "\n",
        "    for doc_idx, doc in enumerate(result['source_documents'], 1):\n",
        "        sentiment = sentiment_pipe(doc.page_content)[0]\n",
        "        label_name = sentiment_labels.get(sentiment['label'], sentiment['label'])\n",
        "\n",
        "        query_results.append({\n",
        "            'query_number': idx,\n",
        "            'query_text': query,\n",
        "            'llm_summary': result['result'],\n",
        "            'retrieved_doc_number': doc_idx,\n",
        "            'emotion': doc.metadata.get('emotion', 'unknown'),\n",
        "            'review_text': doc.page_content,\n",
        "            'sentiment_label': label_name,\n",
        "            'sentiment_score': sentiment['score']\n",
        "        })\n",
        "\n",
        "\n",
        "query_results_path = f\"{STAGE2_DIR}/query_results.csv\"\n",
        "results_df = pd.DataFrame(query_results)\n",
        "results_df.to_csv(query_results_path, index=False)\n",
        "print(f\"\\nğŸ’¾ Query results saved to: {query_results_path}\")\n",
        "print(f\"   Total records: {len(results_df)}\")\n",
        "\n",
        "\n",
        "print(f\"\\nğŸ“Š Results Summary:\")\n",
        "print(f\"   Total queries: {len(all_results)}\")\n",
        "print(f\"   Documents per query: 4\")\n",
        "print(f\"   Total retrieved documents: {len(query_results)}\")\n",
        "\n",
        "print(\"\\nâœ… Query results saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "VTcnUjf2m8Xb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
